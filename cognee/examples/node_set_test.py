import os
import uuid
import logging
import json
from typing import List

import httpx
import asyncio
import cognee
from cognee.api.v1.search import SearchType
from cognee.shared.logging_utils import get_logger, ERROR

# Replace incorrect import with proper cognee.prune module
# from cognee.infrastructure.data_storage import reset_cognee

# Remove incorrect imports and use high-level API
# from cognee.services.cognify import cognify
# from cognee.services.search import search
from cognee.tasks.node_set.apply_node_set import apply_node_set
from cognee.infrastructure.engine.models.DataPoint import DataPoint
from cognee.infrastructure.databases.relational import get_relational_engine
from cognee.modules.data.models import Data

# Configure logging to see detailed output
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Set up httpx client for debugging
httpx.Client(timeout=None)


async def generate_test_node_ids(count: int) -> List[str]:
    """Generate unique node IDs for testing."""
    return [str(uuid.uuid4()) for _ in range(count)]


async def add_text_with_nodeset(text: str, node_set: List[str], text_id: str = None) -> str:
    """Add text with NodeSet to cognee."""
    # Generate text ID if not provided - but note we can't directly set the ID
    if text_id is None:
        text_id = str(uuid.uuid4())

    # Print NodeSet details
    logger.info("Adding text with NodeSet")
    logger.info(f"NodeSet for this text: {node_set}")

    # Use high-level cognee.add() with the correct parameters
    await cognee.add(text, node_set=node_set)
    logger.info("Saved text with NodeSet to database")

    return text_id  # Note: we can't control the actual ID generated by cognee.add()


async def check_data_records():
    """Check for data records in the database to verify NodeSet storage."""
    db_engine = get_relational_engine()
    async with db_engine.get_async_session() as session:
        from sqlalchemy import select

        query = select(Data)
        result = await session.execute(query)
        records = result.scalars().all()

        logger.info(f"Found {len(records)} records in the database")
        for record in records:
            logger.info(f"Record ID: {record.id}, name: {record.name}, node_set: {record.node_set}")
            # Print raw_data_location to see where the text content is stored
            logger.info(f"Raw data location: {record.raw_data_location}")


async def run_simple_node_set_test():
    """Run a simple test of NodeSet functionality."""
    try:
        # Reset cognee data using correct module
        await cognee.prune.prune_data()
        await cognee.prune.prune_system(metadata=True)
        logger.info("Cognee data reset complete")

        # Generate test node IDs for two NodeSets
        # nodeset1 = await generate_test_node_ids(3)
        nodeset2 = await generate_test_node_ids(3)
        nodeset1 = [
            "my_accounting_data",
            "my_set_of_manuals_about_horses",
            "my_elon_musk_secret_file",
        ]

        logger.info(f"Created test node IDs for NodeSet 1: {nodeset1}")
        logger.info(f"Created test node IDs for NodeSet 2: {nodeset2}")

        # Add two texts with NodeSets
        text1 = "Natural Language Processing (NLP) is a field of artificial intelligence focused on enabling computers to understand and process human language. It involves techniques for language analysis, translation, and generation."
        text2 = "Artificial Intelligence (AI) systems are designed to perform tasks that typically require human intelligence. These tasks include speech recognition, decision-making, and language translation."

        text1_id = await add_text_with_nodeset(text1, nodeset1)
        text2_id = await add_text_with_nodeset(text2, nodeset2)
        logger.info(str(text1_id))
        logger.info(str(text2_id))

        # Verify that the NodeSets were stored correctly
        await check_data_records()

        # Run the cognify process to create a knowledge graph
        pipeline_run_id = await cognee.cognify()
        logger.info(f"Cognify process completed with pipeline run ID: {pipeline_run_id}")

        # Skip graph search as the NetworkXAdapter doesn't support Cypher
        logger.info("Skipping graph search as NetworkXAdapter doesn't support Cypher")

        # Search for insights related to the added texts
        search_query = "NLP and AI"
        logger.info(f"Searching for insights with query: '{search_query}'")
        search_results = await cognee.search(
            query_type=SearchType.INSIGHTS, query_text=search_query
        )

        # Extract NodeSet information from search results
        logger.info(f"Found {len(search_results)} search results for '{search_query}':")
        for i, result in enumerate(search_results):
            logger.info(f"Result {i + 1} text: {getattr(result, 'text', 'No text')}")

            # Check for NodeSet and SetNodeId
            node_set = getattr(result, "NodeSet", None)
            set_node_id = getattr(result, "SetNodeId", None)
            logger.info(f"Result {i + 1} - NodeSet: {node_set}, SetNodeId: {set_node_id}")

            # Check id and type
            logger.info(
                f"Result {i + 1} - ID: {getattr(result, 'id', 'No ID')}, Type: {getattr(result, 'type', 'No type')}"
            )

            # Check if this is a document chunk and has is_part_of property
            if hasattr(result, "is_part_of") and result.is_part_of:
                logger.info(
                    f"Result {i + 1} is a document chunk with parent ID: {result.is_part_of.id}"
                )

                # Check if the parent has a NodeSet
                parent_has_nodeset = (
                    hasattr(result.is_part_of, "NodeSet") and result.is_part_of.NodeSet
                )
                logger.info(f"  Parent has NodeSet: {parent_has_nodeset}")
                if parent_has_nodeset:
                    logger.info(f"  Parent NodeSet: {result.is_part_of.NodeSet}")

            # Print all attributes of the result to see what's available
            logger.info(f"Result {i + 1} - All attributes: {dir(result)}")

    except Exception as e:
        logger.error(f"Error in simple NodeSet test: {e}")
        raise


if __name__ == "__main__":
    logger = get_logger(level=ERROR)
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    try:
        loop.run_until_complete(run_simple_node_set_test())
    finally:
        loop.run_until_complete(loop.shutdown_asyncgens())
