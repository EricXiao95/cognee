# ----------------------------------------------------------------------------
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml
#
# ----------------------------------------------------------------------------

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code using: baml-cli generate
# baml-cli is available with the baml package.

import typing
import typing_extensions

from . import stream_types, types
from .runtime import DoNotUseDirectlyCallManager, BamlCallOptions

class LlmResponseParser:
    __options: DoNotUseDirectlyCallManager

    def __init__(self, options: DoNotUseDirectlyCallManager):
        self.__options = options

    def ExtractContentGraph(
        self, llm_response: str, baml_options: BamlCallOptions = {},
    ) -> types.KnowledgeGraph:
        result = self.__options.merge_options(baml_options).parse_response(function_name="ExtractContentGraph", llm_response=llm_response, mode="request")
        return typing.cast(types.KnowledgeGraph, result)

    def ExtractContentGraphGeneric(
        self, llm_response: str, baml_options: BamlCallOptions = {},
    ) -> types.KnowledgeGraph:
        result = self.__options.merge_options(baml_options).parse_response(function_name="ExtractContentGraphGeneric", llm_response=llm_response, mode="request")
        return typing.cast(types.KnowledgeGraph, result)

    def ExtractContentGraphWithAnthropic(
        self, llm_response: str, baml_options: BamlCallOptions = {},
    ) -> types.KnowledgeGraph:
        result = self.__options.merge_options(baml_options).parse_response(function_name="ExtractContentGraphWithAnthropic", llm_response=llm_response, mode="request")
        return typing.cast(types.KnowledgeGraph, result)

    def ExtractContentGraphWithEnvPrompt(
        self, llm_response: str, baml_options: BamlCallOptions = {},
    ) -> types.KnowledgeGraph:
        result = self.__options.merge_options(baml_options).parse_response(function_name="ExtractContentGraphWithEnvPrompt", llm_response=llm_response, mode="request")
        return typing.cast(types.KnowledgeGraph, result)

    

class LlmStreamParser:
    __options: DoNotUseDirectlyCallManager

    def __init__(self, options: DoNotUseDirectlyCallManager):
        self.__options = options

    def ExtractContentGraph(
        self, llm_response: str, baml_options: BamlCallOptions = {},
    ) -> stream_types.KnowledgeGraph:
        result = self.__options.merge_options(baml_options).parse_response(function_name="ExtractContentGraph", llm_response=llm_response, mode="stream")
        return typing.cast(stream_types.KnowledgeGraph, result)

    def ExtractContentGraphGeneric(
        self, llm_response: str, baml_options: BamlCallOptions = {},
    ) -> stream_types.KnowledgeGraph:
        result = self.__options.merge_options(baml_options).parse_response(function_name="ExtractContentGraphGeneric", llm_response=llm_response, mode="stream")
        return typing.cast(stream_types.KnowledgeGraph, result)

    def ExtractContentGraphWithAnthropic(
        self, llm_response: str, baml_options: BamlCallOptions = {},
    ) -> stream_types.KnowledgeGraph:
        result = self.__options.merge_options(baml_options).parse_response(function_name="ExtractContentGraphWithAnthropic", llm_response=llm_response, mode="stream")
        return typing.cast(stream_types.KnowledgeGraph, result)

    def ExtractContentGraphWithEnvPrompt(
        self, llm_response: str, baml_options: BamlCallOptions = {},
    ) -> stream_types.KnowledgeGraph:
        result = self.__options.merge_options(baml_options).parse_response(function_name="ExtractContentGraphWithEnvPrompt", llm_response=llm_response, mode="stream")
        return typing.cast(stream_types.KnowledgeGraph, result)

    